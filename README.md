# Awesome-Agentic-MLLM

ğŸ‘ Welcome to the Awesome-Reasoning-MLLM repository! This repository is a curated collection of the most influential papers, code, dataset, benchmarks, and resources about Reasoning in Multi-Modal Large Language Models (MLLMs) and Vision-Language Models (VLMs).

Feel free to â­ star and fork this repository to keep up with the latest advancements and contribute to the community.


---

## To be classified
* PyVision: Agentic Vision with Dynamic Tooling
* VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual Tool Selection

## Framework

* verl: Volcano Engine Reinforcement Learning for LLMs [[CodeğŸ”§]](https://github.com/volcengine/verl)

---

## Agentic search 

* Visual Agentic Reinforcement Fine-Tuning []
* MMSearch-R1: Incentivizing LMMs to Search [[PaperğŸ“‘]](https://arxiv.org/abs/2506.20670) [[CodeğŸ”§]](https://github.com/EvolvingLMMs-Lab/multimodal-search-r1)
* WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent [[PaperğŸ“‘]](https://arxiv.org/abs/2508.05748) [[CodeğŸ”§]](https://github.com/Alibaba-NLP/WebAgent)
* MiroMind Open Deep Research (https://miromind.ai/blog/miromind-open-deep-research)

<!-- text-->
* From Web Search towards Agentic Deep Research: Incentivizing Search with Reasoning Agents
* [2507] WebSailor: Navigating Super-human Reasoning for Web Agent [[PaperğŸ“‘]](https://arxiv.org/abs/2508.05748) [[CodeğŸ”§]](https://github.com/Alibaba-NLP/WebAgent)
---

## Agentic code

---


GUI VLA




