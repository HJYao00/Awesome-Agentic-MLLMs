# Awesome-Agentic-MLLM

ğŸ‘ Welcome to the Awesome-Reasoning-MLLM repository! This repository is a curated collection of the most influential papers, code, dataset, benchmarks, and resources about Reasoning in Multi-Modal Large Language Models (MLLMs) and Vision-Language Models (VLMs).

Feel free to â­ star and fork this repository to keep up with the latest advancements and contribute to the community.


## ğŸ“’ Table of Contents

- [Awesome-Agentic-MLLM](#awesome-agentic-mllm)
  - [ğŸ“’ Table of Contents](#-table-of-contents)
  - [To be classified](#to-be-classified)
  - [Survey](#survey)
  - [Agentic Search](#agentic-search)
  - [Search Agent](#search-agent)
  - [Agentic code](#agentic-code)
  - [Action](#action)
  - [Framework](#framework)

---

## To be classified
* PyVision: Agentic Vision with Dynamic Tooling
* VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual Tool Selection

---


## Survey
* [2506] From Web Search towards Agentic Deep Research: Incentivizing Search with Reasoning Agents [[PaperğŸ“‘]](https://arxiv.org/abs/2506.18959) [[GithubğŸ“š]](https://github.com/DavidZWZ/Awesome-Deep-Research)



---

## Agentic Search 

* [2505] Visual Agentic Reinforcement Fine-Tuning [[PaperğŸ“‘]](https://arxiv.org/abs/2505.14246) [[CodeğŸ”§]](https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT)
* [2506] MMSearch-R1: Incentivizing LMMs to Search [[PaperğŸ“‘]](https://arxiv.org/abs/2506.20670) [[CodeğŸ”§]](https://github.com/EvolvingLMMs-Lab/multimodal-search-r1)
* [2508] WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent [[PaperğŸ“‘]](https://arxiv.org/abs/2508.05748) [[CodeğŸ”§]](https://github.com/Alibaba-NLP/WebAgent)
* [2508] MiroMind Open Deep Research [[BlogğŸ“]](https://miromind.ai/blog/miromind-open-deep-research)
<!-- text-->
* [2502] Introducing deep research [[BlogğŸ“]](https://openai.com/index/introducing-deep-research/)
* [2507] WebSailor: Navigating Super-human Reasoning for Web Agent [[PaperğŸ“‘]](https://arxiv.org/abs/2508.05748) [[CodeğŸ”§]](https://github.com/Alibaba-NLP/WebAgent)
* [2508] Introducing gpt-oss [[BlogğŸ“]](https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf) [[CodeğŸ”§]](https://github.com/openai/gpt-oss)
* [] 

---

## Search Agent 

* [2401] WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models [[PaperğŸ“‘]](https://arxiv.org/abs/2401.13919) [[CodeğŸ”§]](https://github.com/MinorJerry/WebVoyager)

---

## Agentic code
* [2506] CoRT: Code-integrated Reasoning within Thinking [[PaperğŸ“‘]](https://arxiv.org/abs/2506.09820) [[CodeğŸ”§]](https://github.com/ChengpengLi1003/CoRT)
* [2506] Computational Thinking Reasoning in Large Language Models [[PaperğŸ“‘]](https://arxiv.org/abs/2506.02658)
* [2505] VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use [[PaperğŸ“‘]](https://arxiv.org/abs/2505.19255) [[CodeğŸ”§]](https://github.com/VTool-R1/VTool-R1)
* [2505] Visual Agentic Reinforcement Fine-Tuning [[PaperğŸ“‘]](https://arxiv.org/abs/2505.14246) [[CodeğŸ”§]](https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT)
* [2505] OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning [[PaperğŸ“‘]](https://arxiv.org/abs/2505.08617) [[CodeğŸ”§]](https://github.com/zhaochen0110/OpenThinkIMG)
* [2505] DeepEyes: Incentivizing "Thinking with Images" via Reinforcement Learning [[PaperğŸ“‘]](https://arxiv.org/abs/2505.14362) [[CodeğŸ”§]](https://github.com/Visual-Agent/DeepEyes)
* [2505] Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL [[PaperğŸ“‘]](https://arxiv.org/abs/2505.15436) [[CodeğŸ”§]](https://github.com/xtong-zhang/Chain-of-Focus)
* [2505] Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning [[PaperğŸ“‘]](https://arxiv.org/abs/2505.15966) [[CodeğŸ”§]](https://github.com/TIGER-AI-Lab/Pixel-Reasoner)
* [2505] Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO [[PaperğŸ“‘]](https://arxiv.org/abs/2505.21457) [[CodeğŸ”§]](https://github.com/aim-uofa/Active-o3)
* [2506] VGR: Visual Grounded Reasoning [[PaperğŸ“‘]](https://arxiv.org/abs/2506.11991) 
* [2505] VLM-R3: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought [[PaperğŸ“‘]](https://arxiv.org/abs/2505.16192) 
* [2505] ARGUS: Vision-Centric Reasoning with Grounded Chain-of-Thought [[PaperğŸ“‘]](https://arxiv.org/abs/2505.23766) 
* [2505] Ground-R1: Incentivizing Grounded Visual Reasoning via Reinforcement Learning [[PaperğŸ“‘]](https://arxiv.org/abs/2505.20272) [[CodeğŸ”§]](https://github.com/zzzhhzzz/Ground-R1)
* [2506] Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing [[PaperğŸ“‘]](https://arxiv.org/abs/2506.09965) [[CodeğŸ”§]](https://github.com/AntResearchNLP/ViLaSR)

---


## Action

* [2508] MolmoAct: Action Reasoning Models that can Reason in Space [[PaperğŸ“‘]](https://arxiv.org/abs/2508.07917)  [[CodeğŸ”§]](https://github.com/allenai/MolmoAct) [[BlogğŸ“]](https://allenai.org/blog/molmoact)
* [2507] ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning [[PaperğŸ“‘]](https://arxiv.org/abs/2507.16815) [[Project ğŸŒ](https://jasper0314-huang.github.io/thinkact-vla/)] 
* [2506] VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning [[PaperğŸ“‘]](https://arxiv.org/abs/2506.17221) [[BlogğŸ“]](https://vlnr1.github.io/) [[CodeğŸ”§]](https://github.com/Qi-Zhangyang/GPT4Scene-and-VLN-R1)

## Framework

* verl: Volcano Engine Reinforcement Learning for LLMs [[CodeğŸ”§]](https://github.com/volcengine/verl)

