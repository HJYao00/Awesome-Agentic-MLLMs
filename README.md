# Awesome-Agentic-MLLM

ğŸ‘ Welcome to the Awesome-Agentic-MLLM repository! This is a curated collection of influential papers, code, datasets, benchmarks, and resources exploring the emerging domain of agentic capabilities in Multi-Modal Large Language Models (MLLMs) and Vision-Language Models (VLMs).

Feel free to â­ star and fork this repository to keep up with the latest advancements and contribute to the community.


## ğŸ“’ Table of Contents

- [Awesome-Agentic-MLLM](#awesome-agentic-mllm)
  - [ğŸ“’ Table of Contents](#-table-of-contents)
  - [To be classified](#to-be-classified)
  - [Survey](#survey)
  - [Agentic Search](#agentic-search)
  - [Search Agent](#search-agent)
  - [Agentic Code](#agentic-code)
  - [Action](#action)
  - [Framework](#framework)
  - [Application]
---

## To be classified
* PyVision: Agentic Vision with Dynamic Tooling
* VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual Tool Selection
* Agentic Reasoning: A Streamlined Framework for Enhancing LLM Reasoning with Agentic Tools (prompt-based)
---


## Survey
* [2308] A Survey on Large Language Model based Autonomous Agents [[PaperğŸ“‘]](https://arxiv.org/abs/2308.11432) [[ProjectğŸ“š]](https://github.com/DavidZWZ/Awesome-Deep-Research)
* [2402] Large Multimodal Agents: A Survey [[PaperğŸ“‘]](https://arxiv.org/abs/2402.15116)
* [2501] Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG [[PaperğŸ“‘]](https://arxiv.org/abs/2501.09136)
* [2502] Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation [[PaperğŸ“‘]](https://arxiv.org/abs/2502.08826)
* [2503] Agentic Large Language Models, a survey [[PaperğŸ“‘]](https://arxiv.org/abs/2503.23037)
* [2504] From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs [[PaperğŸ“‘]](https://arxiv.org/pdf/2504.15965)
* [2505] Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions [[PaperğŸ“‘]](https://arxiv.org/abs/2505.00675)
* [2506] From Web Search towards Agentic Deep Research: Incentivizing Search with Reasoning Agents [[PaperğŸ“‘]](https://arxiv.org/abs/2506.18959) [[ProjectğŸ“š]](https://github.com/DavidZWZ/Awesome-Deep-Research)
* [2507] Evaluation and Benchmarking of LLM Agents: A Survey [[PaperğŸ“‘]](https://arxiv.org/pdf/2507.21504)
* [2508] AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities [[PaperğŸ“‘]](https://arxiv.org/pdf/2508.11126)
* [2508] Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction [[PaperğŸ“‘]](https://arxiv.org/pdf/2508.05294)
* [2508] A Survey on Agent Workflow â€“ Status and Future [[PaperğŸ“‘]](https://arxiv.org/pdf/2508.01186)

---

## Agentic Reasoning

### Prompt-based

### SFT-based

### RL-based

* [2506] GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning [[PaperğŸ“‘]](https://arxiv.org/pdf/2506.16141) [[CodeğŸ”§]](https://github.com/TencentARC/GRPO-CARE)


---

## Agentic Reflection

* [2410] ReflecTool: Towards Reflection-Aware Tool-Augmented Clinical Agents [[PaperğŸ“‘]](https://arxiv.org/abs/2410.17657) [[CodeğŸ”§]](https://github.com/BlueZeros/ReflecTool)
* [2411] Self-Corrected Multimodal Large Language Model for Robot Manipulation and Reflection [[PaperğŸ“‘]](https://openreview.net/forum?id=TLWbNfbkxj) 
* [2411] Vision-Language Models Can Self-Improve Reasoning via Reflection [[PaperğŸ“‘]](https://arxiv.org/abs/2411.00855) [[CodeğŸ”§]](https://github.com/njucckevin/MM-Self-Improve)
* [2412] Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search [[PaperğŸ“‘]](https://arxiv.org/pdf/2412.18319) [[CodeğŸ”§]](https://github.com/HJYao00/Mulberry)
* [2503] V-Stylist: Video Stylization via Collaboration and Reflection of MLLM Agents [[PaperğŸ“‘]](https://arxiv.org/abs/2503.12077) [[CodeğŸ”§]]()
* [2504] MASR: Self-Reflective Reasoning through Multimodal Hierarchical Attention Focusing for Agent-based Video Understanding [[PaperğŸ“‘]](https://arxiv.org/abs/2504.17213)
* [2504] VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning [[PaperğŸ“‘]](https://arxiv.org/abs/2504.08837) [[CodeğŸ”§]](https://tiger-ai-lab.github.io/VL-Rethinker/)
* [2505] Training-Free Reasoning and Reflection in MLLMs [[PaperğŸ“‘]](https://arxiv.org/abs/2505.16151) [[CodeğŸ”§]](https://iip.whu.edu.cn/frank/index.html)
* [2506] SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning [[PaperğŸ“‘]](https://arxiv.org/abs/2506.01713) [[CodeğŸ”§]](https://srpo.pages.dev/)
* [2507] Look-Back: Implicit Visual Re-focusing in MLLM Reasoning [[PaperğŸ“‘]](https://arxiv.org/pdf/2507.03019) [[CodeğŸ”§]](https://github.com/PKU-YuanGroup/Look-Back)


---

## Agentic Memory
* [2305] MemoryBank: Enhancing Large Language Models with Long-Term Memory [[PaperğŸ“‘]](https://arxiv.org/abs/2305.10250) [[CodeğŸ”§]](https://github.com/zhongwanjun/MemoryBank-SiliconFriend)
* [2312] Empowering Working Memory for Large Language Model Agents [[PaperğŸ“‘]](https://arxiv.org/pdf/2312.17259)
* [2502] A-Mem: Agentic Memory for LLM Agents [[PaperğŸ“‘]](https://arxiv.org/pdf/2502.12110) [[CodeğŸ”§]](https://github.com/WujiangXu/A-mem)
* [2504] Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory [[PaperğŸ“‘]](https://arxiv.org/abs/2504.19413) [[CodeğŸ”§]](https://mem0.ai/research)
* [2506] A Walk to Remember: Mllm Memory-Driven Visual Navigation [[PaperğŸ“‘]](https://ieeexplore.ieee.org/abstract/document/11078086)
* [2506] MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents [[PaperğŸ“‘]](https://arxiv.org/abs/2506.15841) [[CodeğŸ”§]](https://github.com/MIT-MI/MEM1)
* [2507] Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions [[PaperğŸ“‘]](https://arxiv.org/pdf/2507.05257) [[CodeğŸ”§]](https://github.com/HUST-AI-HYZ/MemoryAgentBench)
* [2507] MemOS: A Memory OS for AI System [[PaperğŸ“‘]](https://arxiv.org/abs/2507.03724) [[CodeğŸ”§]](https://github.com/MemTensor/MemOS)
* [2507] MemTool: Optimizing Short-Term Memory Management for Dynamic Tool Calling in LLM Agent Multi-Turn Conversations [[PaperğŸ“‘]](https://arxiv.org/abs/2507.21428)
* [2507] MIRIX: Multi-Agent Memory System for LLM-Based Agents [[PaperğŸ“‘]](https://arxiv.org/pdf/2507.07957) [[CodeğŸ”§]](https://mirix.io/)
* [2507] Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents [[PaperğŸ“‘]](https://arxiv.org/abs/2507.22925)
* [2508] Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning [[PaperğŸ“‘]](https://arxiv.org/abs/2508.19828)
* [2508] Intrinsic Memory Agents: Heterogeneous Multi-Agent LLM Systems through Structured Contextual Memory [[PaperğŸ“‘]](https://arxiv.org/abs/2508.08997
* [2508] MMS: Multiple Memory Systems for Enhancing the Long-term Memory of Agent [[PaperğŸ“‘]](https://arxiv.org/abs/2508.15294)

----

## Agentic Search 

* [2505] VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning [[PaperğŸ“‘]](https://arxiv.org/abs/2505.22019) [[CodeğŸ”§]](https://github.com/Alibaba-NLP/VRAG)
* [2505] Visual Agentic Reinforcement Fine-Tuning [[PaperğŸ“‘]](https://arxiv.org/abs/2505.14246) [[CodeğŸ”§]](https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT)
* [2506] MMSearch-R1: Incentivizing LMMs to Search [[PaperğŸ“‘]](https://arxiv.org/abs/2506.20670) [[CodeğŸ”§]](https://github.com/EvolvingLMMs-Lab/multimodal-search-r1)
* [2508] WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent [[PaperğŸ“‘]](https://arxiv.org/abs/2508.05748) [[CodeğŸ”§]](https://github.com/Alibaba-NLP/WebAgent)
* [2508] MiroMind Open Deep Research [[BlogğŸ“]](https://miromind.ai/blog/miromind-open-deep-research)
<!-- text-->
* [2502] Introducing deep research [[BlogğŸ“]](https://openai.com/index/introducing-deep-research/)
* [2507] WebSailor: Navigating Super-human Reasoning for Web Agent [[PaperğŸ“‘]](https://arxiv.org/abs/2508.05748) [[CodeğŸ”§]](https://github.com/Alibaba-NLP/WebAgent)
* [2508] Introducing gpt-oss [[BlogğŸ“]](https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf) [[CodeğŸ”§]](https://github.com/openai/gpt-oss)
* [2508] Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL [[PaperğŸ“‘]](https://arxiv.org/abs/2508.07976) [[CodeğŸ”§]](https://github.com/inclusionAI/ASearcher?tab=readme-ov-file)

---



## Search Agent 

* [2112] WebGPT: WebGPT: Browser-assisted question-answering with human feedback [[PaperğŸ“‘]](https://arxiv.org/pdf/2112.09332)
* [2207] WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents [[PaperğŸ“‘]](https://arxiv.org/pdf/2207.01206) [[CodeğŸ”§]](https://github.com/princeton-nlp/WebShop)
* [2306] MIND2WEB: Towards a Generalist Agent for the Web [[PaperğŸ“‘]](https://arxiv.org/abs/2306.06070)
* [2310] WebWISE: Web Interface Control and Sequential Exploration with Large Language Models [[PaperğŸ“‘]](https://arxiv.org/pdf/2310.16042)
* [2401] VisualWebArena: Evaluating Multimodal Agents on Realistic Visually Grounded Web Tasks [[PaperğŸ“‘]](https://arxiv.org/abs/2401.13649) [[CodeğŸ”§]](https://jykoh.com/vwa)
* [2401] WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models [[PaperğŸ“‘]](https://arxiv.org/abs/2401.13919) [[CodeğŸ”§]](https://github.com/MinorJerry/WebVoyager)
* [2401] GPT-4V(ision) is a Generalist Web Agent, if Grounded [[PaperğŸ“‘]](https://arxiv.org/pdf/2401.01614) [[CodeğŸ”§]](https://github.com/OSU-NLP-Group/SeeAct)
* [2408] PhishAgent: A Robust Multimodal Agent for Phishing Webpage Detection [[PaperğŸ“‘]](https://arxiv.org/abs/2408.10738)
* [2410] OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization [[PaperğŸ“‘]](https://arxiv.org/abs/2410.19609) [[CodeğŸ”§]](https://github.com/MinorJerry/OpenWebVoyager)
* [2410] Multimodal Auto Validation For Self-Refinement in Web Agents [[PaperğŸ“‘]] (https://arxiv.org/abs/2410.00689)
* [2411] AdaptAgent: Adapting Multimodal Web Agents with Few-Shot Learning from Human Demonstrations [[PaperğŸ“‘]](https://arxiv.org/abs/2411.13451)

* [2501] Visual RAG: Expanding MLLM visual knowledge without fine-tuning


---

## Code Agent


---

## Agentic Code
* [2506] CoRT: Code-integrated Reasoning within Thinking [[PaperğŸ“‘]](https://arxiv.org/abs/2506.09820) [[CodeğŸ”§]](https://github.com/ChengpengLi1003/CoRT)
* [2506] Computational Thinking Reasoning in Large Language Models [[PaperğŸ“‘]](https://arxiv.org/abs/2506.02658)
* [2505] VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use [[PaperğŸ“‘]](https://arxiv.org/abs/2505.19255) [[CodeğŸ”§]](https://github.com/VTool-R1/VTool-R1)
* [2505] Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning [[PaperğŸ“‘]](https://arxiv.org/abs/2505.16410) [[CodeğŸ”§]](https://github.com/dongguanting/Tool-Star)
* [2504] ReTool: Reinforcement Learning for Strategic Tool Use in LLMs [[PaperğŸ“‘]](https://arxiv.org/abs/2504.11536) [[CodeğŸ”§]](https://github.com/ReTool-RL/ReTool)
* [2504] SQL-R1: Training Natural Language to SQL Reasoning Model By Reinforcement Learning [[PaperğŸ“‘]](https://arxiv.org/abs/2504.08600) [[CodeğŸ”§]](https://github.com/DataArcTech/SQL-R1) 
* [2505] Visual Agentic Reinforcement Fine-Tuning [[PaperğŸ“‘]](https://arxiv.org/abs/2505.14246) [[CodeğŸ”§]](https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT)
* [2506] ComfyUI-R1: Exploring Reasoning Models for Workflow Generation [[PaperğŸ“‘]](https://arxiv.org/abs/2506.09790) [[CodeğŸ”§]](https://github.com/AIDC-AI/ComfyUI-Copilot)
* [2508] Thyme: Think Beyond Images [[PaperğŸ“‘]](https://arxiv.org/pdf/2508.11630) [[CodeğŸ”§]](https://github.com/yfzhang114/Thyme)
* [2505] OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning [[PaperğŸ“‘]](https://arxiv.org/abs/2505.08617) [[CodeğŸ”§]](https://github.com/zhaochen0110/OpenThinkIMG)
* [2505] DeepEyes: Incentivizing "Thinking with Images" via Reinforcement Learning [[PaperğŸ“‘]](https://arxiv.org/abs/2505.14362) [[CodeğŸ”§]](https://github.com/Visual-Agent/DeepEyes)
* [2505] Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL [[PaperğŸ“‘]](https://arxiv.org/abs/2505.15436) [[CodeğŸ”§]](https://github.com/xtong-zhang/Chain-of-Focus)
* [2505] Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning [[PaperğŸ“‘]](https://arxiv.org/abs/2505.15966) [[CodeğŸ”§]](https://github.com/TIGER-AI-Lab/Pixel-Reasoner)
* [2505] Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO [[PaperğŸ“‘]](https://arxiv.org/abs/2505.21457) [[CodeğŸ”§]](https://github.com/aim-uofa/Active-o3)
* [2506] VGR: Visual Grounded Reasoning [[PaperğŸ“‘]](https://arxiv.org/abs/2506.11991) 
* [2505] VLM-R3: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought [[PaperğŸ“‘]](https://arxiv.org/abs/2505.16192) 
* [2505] ARGUS: Vision-Centric Reasoning with Grounded Chain-of-Thought [[PaperğŸ“‘]](https://arxiv.org/abs/2505.23766) 
* [2505] Ground-R1: Incentivizing Grounded Visual Reasoning via Reinforcement Learning [[PaperğŸ“‘]](https://arxiv.org/abs/2505.20272) [[CodeğŸ”§]](https://github.com/zzzhhzzz/Ground-R1)
* [2506] Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing [[PaperğŸ“‘]](https://arxiv.org/abs/2506.09965) [[CodeğŸ”§]](https://github.com/AntResearchNLP/ViLaSR)

---

## GUI Agent

* [] You Only Look at Screens: Multimodal Chain-of-Action Agents


----

## Agentic GUI


----

## VLA Action

* [2508] MolmoAct: Action Reasoning Models that can Reason in Space [[PaperğŸ“‘]](https://arxiv.org/abs/2508.07917)  [[CodeğŸ”§]](https://github.com/allenai/MolmoAct) [[BlogğŸ“]](https://allenai.org/blog/molmoact)
* [2507] ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning [[PaperğŸ“‘]](https://arxiv.org/abs/2507.16815) [[Project ğŸŒ](https://jasper0314-huang.github.io/thinkact-vla/)] 
* [2506] VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning [[PaperğŸ“‘]](https://arxiv.org/abs/2506.17221) [[BlogğŸ“]](https://vlnr1.github.io/) [[CodeğŸ”§]](https://github.com/Qi-Zhangyang/GPT4Scene-and-VLN-R1)

----

## Benchmark
* [] Agent security bench (asb): Formalizing and benchmarking attacks and defenses in llm-based agents
* [2508] Dissecting Tool-Integrated Reasoning: An Empirical Study and Analysis [[PaperğŸ“‘]](https://arxiv.org/pdf/2508.15754) 

----

## Application
* 

## Framework

* verl: Volcano Engine Reinforcement Learning for LLMs [[CodeğŸ”§]](https://github.com/volcengine/verl)
* TRL: Transformer Reinforcement Learning [[CodeğŸ”§]](https://github.com/huggingface/trl)
* Open R1 [[CodeğŸ”§]](https://github.com/huggingface/open-r1)
* OpenRLHF [[CodeğŸ”§]](https://github.com/OpenRLHF/OpenRLHF)
* Multimodal Open R1 [[CodeğŸ”§]](https://github.com/EvolvingLMMs-Lab/open-r1-multimodal)
* RLFactory: Easy and Efficient RL Training [[CodeğŸ”§]](https://github.com/Simple-Efficient/RL-Factory)
* Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning [[CodeğŸ”§]](https://github.com/Unakar/Logic-RL)
* EasyR1: An Efficient, Scalable, Multi-Modality RL Training Framework [[CodeğŸ”§]](https://github.com/hiyouga/EasyR1)
* Simple-R1: Simple Reinforcement Learning for Reasoning [[CodeğŸ”§]](https://github.com/hkust-nlp/simpleRL-reason)
* Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond [[CodeğŸ”§]](https://github.com/Qihoo360/Light-R1)
* RL2: Ray Less Reinforcement Learning [[CodeğŸ”§]](https://github.com/ChenmienTan/RL2/tree/main)
* OAT [[CodeğŸ”§]](https://github.com/sail-sg/oat)
* R1-V: Reinforcing Super Generalization Ability in Vision Language Models with Less Than $3 [[CodeğŸ”§]](https://github.com/StarsfieldAI/R1-V)
* Visual-RFT: Visual Reinforcement Fine-Tuning [[CodeğŸ”§]](https://github.com/Liuziyu77/Visual-RFT)
* rLLM (DeepScaleR): Reinforcement Learning for Language Agents [[CodeğŸ”§]](https://github.com/rllm-org/rllm)
* Search-R1: Train your LLMs to reason and call a search engine with reinforcement learning [[CodeğŸ”§]](https://github.com/PeterGriffinJin/Search-R1)
* Multimodal-Search-R1: Incentivizing LMMs to Search [[CodeğŸ”§]](https://github.com/EvolvingLMMs-Lab/multimodal-search-r1)
* Agent Lightning [[CodeğŸ”§]](https://github.com/microsoft/agent-lightning)
* ROLL: Reinforcement Learning Optimization for Large-Scale Learning [[CodeğŸ”§]](https://github.com/alibaba/ROLL)
* RAGEN: Training Agents by Reinforcing Reasoning [[CodeğŸ”§]](https://github.com/RAGEN-AI/RAGEN)
* MARTI: A Framework for LLM-based Multi-Agent Reinforced Training and Inference [[CodeğŸ”§]](https://github.com/TsinghuaC3I/MARTI)
* SkyRL: A Modular Full-stack RL Library for LLMs [[CodeğŸ”§]](https://github.com/NovaSky-AI/SkyRL)
* AReaL: Ant Reasoning Reinforcement Learning for LLMs [[CodeğŸ”§]](https://github.com/inclusionAI/AReaL)
* MiroRL: An MCP-first Reinforcement Learning Framework for Deep Research Agent [[CodeğŸ”§]](https://github.com/MiroMindAI/MiroRL)
* AgentFly: Training scalable LLM agents with RL [[CodeğŸ”§]](https://github.com/Agent-One-Lab/AgentFly)
* AWorld: The Agent Runtime for Self-Improvement [[CodeğŸ”§]](https://github.com/inclusionAI/AWorld/tree/main)

